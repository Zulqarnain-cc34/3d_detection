{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP1: 3D Open-Vocabulary Object Detection Pipeline\n",
    "\n",
    "**Interactive Testing Notebook**\n",
    "\n",
    "This notebook demonstrates the complete RGB-only 3D object detection pipeline:\n",
    "- **Stage A**: Open-vocabulary 2D detection (YOLO-World)\n",
    "- **Stage B**: Monocular depth estimation (Depth Anything V2)\n",
    "- **Stage C**: 3D geometric projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install -q torch torchvision ultralytics transformers pillow opencv-python matplotlib supervision pyyaml requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import SP1Pipeline, PipelineVisualizer\n",
    "\n",
    "# Initialize pipeline (use 'cuda:0' if GPU available)\n",
    "pipeline = SP1Pipeline(\n",
    "    detector_model='yolov8s-world',\n",
    "    device='cpu',  # Change to 'cuda:0' for GPU\n",
    "    confidence_threshold=0.25\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample indoor scene\n",
    "image_url = \"https://images.unsplash.com/photo-1586023492125-27b2c045efd7?w=800\"\n",
    "\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "image_np = np.array(image)\n",
    "\n",
    "print(f\"Image shape: {image_np.shape}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image_np)\n",
    "plt.title(\"Test Image: Indoor Room Scene\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run 3D Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objects to detect (open-vocabulary - you can use any object names!)\n",
    "query_classes = [\n",
    "    \"chair\", \"sofa\", \"table\", \"lamp\", \n",
    "    \"tv\", \"door\", \"window\", \"plant\", \"pillow\"\n",
    "]\n",
    "\n",
    "print(f\"Searching for: {query_classes}\")\n",
    "print(\"\\nRunning pipeline...\")\n",
    "\n",
    "# Run detection\n",
    "result = pipeline.detect(image_np, query_classes)\n",
    "\n",
    "# Print summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "viz = PipelineVisualizer()\n",
    "\n",
    "fig = viz.visualize_pipeline_result(\n",
    "    image=image_np,\n",
    "    detections_2d=result.detections_2d,\n",
    "    depth_result=result.depth_result,\n",
    "    detections_3d=result.detections_3d,\n",
    "    title=\"SP1 3D Detection Pipeline Results\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect Individual Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DETAILED 3D DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, det in enumerate(result.detections_3d):\n",
    "    print(f\"\\n[{i+1}] {det.class_name.upper()}\")\n",
    "    print(f\"    Confidence: {det.confidence:.2%}\")\n",
    "    print(f\"    3D Center: X={det.center[0]:.2f}m, Y={det.center[1]:.2f}m, Z={det.center[2]:.2f}m\")\n",
    "    print(f\"    Dimensions: W={det.dimensions[0]:.2f}m, H={det.dimensions[1]:.2f}m, D={det.dimensions[2]:.2f}m\")\n",
    "    print(f\"    Distance from camera: {det.center[2]:.2f} meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Navigation Waypoint Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate navigation waypoint to a target object\n",
    "target_object = \"sofa\"  # Try different objects!\n",
    "\n",
    "waypoint = pipeline.get_waypoint(\n",
    "    image_np, \n",
    "    target_object=target_object,\n",
    "    offset_distance=0.5  # Stop 0.5m from object\n",
    ")\n",
    "\n",
    "if waypoint:\n",
    "    print(f\"\\nüéØ Navigation waypoint to '{target_object}':\")\n",
    "    print(f\"   Object position: {waypoint['object_position']}\")\n",
    "    print(f\"   Waypoint (stop here): {waypoint['waypoint_position']}\")\n",
    "    print(f\"   Distance: {waypoint['distance_to_object']:.2f}m\")\n",
    "    print(f\"   Confidence: {waypoint['confidence']:.2%}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå '{target_object}' not found in scene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Try Custom Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own queries - open vocabulary means any object name works!\n",
    "custom_queries = [\"couch\", \"coffee table\", \"potted plant\", \"cushion\"]\n",
    "\n",
    "print(f\"Custom query: {custom_queries}\\n\")\n",
    "\n",
    "custom_result = pipeline.detect(image_np, custom_queries)\n",
    "\n",
    "for det in custom_result.detections_3d:\n",
    "    print(f\"Found: {det.class_name} at {det.center[2]:.2f}m depth (conf: {det.confidence:.2%})\")\n",
    "\n",
    "if not custom_result.detections_3d:\n",
    "    print(\"No objects detected with these queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Depth Map Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.depth_estimator import DepthMapVisualizer\n",
    "\n",
    "# Visualize depth map with different colormaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "colormaps = ['plasma', 'viridis', 'magma']\n",
    "for ax, cmap in zip(axes, colormaps):\n",
    "    depth_vis = DepthMapVisualizer.to_colormap(result.depth_result.depth_map, cmap)\n",
    "    ax.imshow(depth_vis)\n",
    "    ax.set_title(f\"Depth Map ({cmap})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f\"Depth Range: {result.depth_result.min_depth:.2f}m - {result.depth_result.max_depth:.2f}m\", \n",
    "             fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Depth statistics\n",
    "print(f\"\\nDepth Statistics:\")\n",
    "print(f\"  Min depth: {result.depth_result.min_depth:.2f}m\")\n",
    "print(f\"  Max depth: {result.depth_result.max_depth:.2f}m\")\n",
    "print(f\"  Mean depth: {result.depth_result.mean_depth:.2f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance benchmark\n",
    "print(\"Running benchmark (5 iterations)...\\n\")\n",
    "\n",
    "times = []\n",
    "for i in range(5):\n",
    "    r = pipeline.detect(image_np, query_classes)\n",
    "    times.append(r.total_time_ms)\n",
    "    print(f\"  Run {i+1}: {r.total_time_ms:.1f}ms\")\n",
    "\n",
    "avg_time = np.mean(times)\n",
    "fps = 1000 / avg_time\n",
    "\n",
    "print(f\"\\nüìä Benchmark Results:\")\n",
    "print(f\"   Average time: {avg_time:.1f}ms\")\n",
    "print(f\"   FPS: {fps:.2f}\")\n",
    "print(f\"   Breakdown:\")\n",
    "print(f\"     - Detection: {r.detection_time_ms:.1f}ms\")\n",
    "print(f\"     - Depth: {r.depth_time_ms:.1f}ms\")\n",
    "print(f\"     - Projection: {r.projection_time_ms:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Export results to JSON\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "result.save_json('outputs/detection_results.json')\n",
    "print(\"Results saved to outputs/detection_results.json\")\n",
    "\n",
    "# Preview JSON structure\n",
    "print(\"\\nJSON Structure:\")\n",
    "print(json.dumps(result.to_dict(), indent=2)[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test with Your Own Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your own image\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# your_image = np.array(Image.open(list(uploaded.keys())[0]).convert('RGB'))\n",
    "\n",
    "# Or use a URL:\n",
    "# your_url = \"https://your-image-url.jpg\"\n",
    "# your_image = np.array(Image.open(BytesIO(requests.get(your_url).content)).convert('RGB'))\n",
    "\n",
    "# Then run:\n",
    "# your_result = pipeline.detect(your_image, ['chair', 'table', 'person'])\n",
    "# print(your_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Pipeline Test Complete!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Open-Vocabulary Detection**: Query any object by text description\n",
    "2. **RGB-Only Depth**: No depth sensor needed - monocular depth from single image\n",
    "3. **3D Localization**: Get actual metric positions (meters) in camera frame\n",
    "4. **Navigation Ready**: Generate waypoints for robot navigation\n",
    "\n",
    "### Next Steps for Deployment:\n",
    "\n",
    "1. **TensorRT Optimization**: Convert models to FP16/INT8 for Jetson Orin\n",
    "2. **Camera Calibration**: Use actual camera intrinsics for your robot\n",
    "3. **ROS Integration**: Wrap pipeline in ROS2 node for navigation stack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
